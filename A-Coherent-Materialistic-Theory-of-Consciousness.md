_This is a work in progress._

_Please come back later._

Abstract:
- Consciousness is produced by a semiotic multi-step planner state machine with meta management. 

This article is interested in resolving the most perplexing question of consciousness..... In an attempt at being as precise as possible, I use the term "subjective experience". Necessary to deflect from our own intuitions and biases and connotations associated with words used in discussion of Cs. Here, we are interested in understanding the mechanisms underlying subjective experience. What produces subjective experience? Why might some structures produce subjective experience while others might not? What are the sufficient and necessary conditions for subjective experience?

Many extent theories invoke non-Materialistic mechanisms. Here I present the case that CS can be fully explained through materialistic means. It just requires that we understand some of how General Intelligence works. 

Some provide mechanistic explanations, but often either only at a philosophical level or only provide part of the story. In many respects, the skill of a researcher in this area is to sieve through the many partial stories and see how they can be linked together. This paper attempts just that. None of the ideas are entirely new. But I believe this to be the most comprehensive attempt to combine theories together at the most detailed level. 


---

# Part I - Introduction

I believe that three things go hand-in-hand: general intelligence, meta-management, and consciousness. Further, understanding these systems must proceed in that order. Thus, in order to understand why we have subjective experience, we need to begin to develop a theory of general intelligence and of the processes that restrain it from cascading into chaos. This article presents an end-to-end argument following that thread. The entire argument is first presented here in brief, with background explanations omitted for the sake of brevity. The rest of the article is devoted to detailed explanations of each of the logical steps in the argument thread.

Any computational system is limited in the complexity that it can handle within a single execution of its computational process. For embodied agents, this appears as a limit on the environmental complexity that they can sufficiently model and respond to within a single executional iteration. For more complex problems, multiple iterations of processing are required in order to determine the next physical action. Such recurrency in processing may for example entail further analysis of the environment in order to better model its state; or consideration of alternative action plans. In biology, this provides scope for evolutionary pressures to trade off between a more energy hungry complex brain and a simpler less energy intensive one that takes longer to make some decisions. Van Bergen & Kriegeskorte (2020) make the case that recurrency is indeed employed in biology for that very reason.

During the execution of a _multi-iteration_ controller within an embodied agent, its control process (CP) passes through an internal state trajectory that is only occasionally associated with interaction with the physical environment. That internal state trajectory can become increasingly disassociated with the physical environment the more complex the problem space and the longer time required for deliberation. If the multi-iteration processor must also learn through reinforcement then it is likely to exhibit chaotic and unproductive behaviour, particularly so during the earliest stages of learning. Reinforcement from the environment may be too sparse for efficient learning to take place, and simple rules that penalise longer deliberation time may be insufficiently advanced to cater for the complexity of problem domains that the agent may be faced with. Explicit meta-management processes are required to observe the control process, to model its behaviours, to track its success rate, to act upon it to prevent chaotic behaviours that could harm the agent, and to participate in providing rewards and penalties with more advanced problem-domain aware knowledge than just a simple penalty for deliberation time.

In a complex biological brain that can operate its body effectively within the real world, including all the complexity of modern human life, the systems and processes required to model and understand the environment and to interact with them are immense. It turns out that the systems required to effectively carry out meta-management are similarly complex. More importantly, the systems required for meta-management are similar to those required for primary control: observing, modelling, inference, planning, sequencing, controlling. Additionally, the domain knowledge required by the meta-management process in order to effectively meta-manage the control process is often strongly associated with the domain knowledge employed by the primary control process at the time. The level of overlap between primary control and meta-management implies a radical solution: that the control process meta-manages itself.

In order to meta-manage itself, the control process needs to observe itself. This can be achieved through a _meta-management feedback loop_ that observes the state of the control process, observes the recent trajectory of the control process, distils that into a high-order representation with lower dimensionality, and makes it available to the control process as a sensory input signal. Thus, the fact that we have awareness of some aspect of our own mental state is a direct result of the need for meta-management. And that meta-management feedback loop is a 6th sense.

The existence of the meta-management feedback loop does not alone explain subjective experience. Two more ingredients are required: interpretation and meaning. In a complex organism such as a human being, the brain maintains _schema_ that represent and track the characteristics of different aspects of the individual. This includes the body schema, which models and tracks the location and orientation of the limbs, their abilities, and whether any injuries have been acquired. This includes schema about regularly encountered external things, such as is required to mentally track the location and orientation of the wheels while driving a car over potholes. With the introduction of the meta-management feedback loop, this also includes schema for tracking the state and capabilities of the mind. And, importantly, it includes a strongly developed sense of self vs other. All external and internal sensory inputs (including the meta-management feedback loop), all schema states, are labelled as to their source. Different source labels imply significantly different meaning in terms of, for example, the level to which the agent can affect that state.

External sensory inputs, meaning association, and feedback loop together are interpreted by the brain and a decision made about the next action. Information about that action and/or the mental state that produced it is available via the feedback loop in the next iteration of processing. This creates a continuous cyclic stream of ever changing inputs, states and actions. As the control process decides to perform some body action, knowledge of that chosen action is immediately available as a sensory input before the action is even started. As the control process chooses to deliberate further on a problem at hand, knowledge of that deliberation and is immediately available. At every moment in time, the control process can choose to attend to external matters, to continuation of the current deliberation problem at hand, or to the very feedback sense that it receives continuously while it is usually distracted doing other things. If the control process stops to consider its own feedback sense, compares that to memory of recent deliberations, compares that to its schema of self vs other, it necessarily concludes that it has its own "stream of thought". That stream of thought is subjective experience.

The metaphor of a _philosophical zombie_ was introduced to hypothesise a human-like individual that has all the behavioural characteristics of a human, including voicing that it is conscious, without actually experiencing subjective experience. A philosophical zombie is _behaviourally indistinguishable_ from a human with consciousness. I argue that, if the zombie employs the mechanisms given in the explanation above (and outlined in more detail in the chapters that follow), then it is also _computationally indistinguishable_ from a human with consciousness. In other words, if we were to assume an ability to tap into all of the inner state and workings of a brain, and if we were to compare the philosophical zombie from the conscious human, we would find no difference. At that point I argue that the zombie is not a zombie after all, but instead is a fully conscious human being having subjective experience.

Finally, I argue that our disgruntlement with such an explanation is not an _explanatory gap_ [citation], but an _intuitional gap_. Nagal famously pointed out that we have no conception of and no way of developing a conception of what it is like to be anything other than ourselves [citation]. But that does not stop us making assumptions about what should and should not experience subjective experience. It took us a very long time to accept that animals could have any form of consciousness and subjective experience, and likely many still deny that outcome [details, citations]. If we have no way to conceive of the experience held by an animal, why should we be so adamant about its properties? The answer simply is our deluded intuition. Our brains excel at finding patterns and extrapolating from them. This works well when the physical environment around us is there to provide an error signal. But when no error signal is available, we are prone to delusion. Our minds create such a strong sense of self vs other by keying that information into every sensory signal that we ever receive, so that our senses seem to take on an extra quality of realness, of subjectiveness, of _qualia_ [citation]. That seeming extra quality is further processed by the same system that produced it, reinforcing the delusion that the qualia is something extra, beyond mere sensory information. And thus we are deluded into the intuition that subjective experience is somehow more than can be produced by mere computational processing.


---


# Part II - Background

What is consciousness? This a question that has been considered for a long time. When Chalmers now famously re-phrased it as the "hard problem of consciousness", he went so far as to claim that no currently known mechanism can produce the phenomenon of subjective experience.

_Todo later: extend out as suitable intro for someone totally new to the question._


# II.1 Theories of Consciousness

Many theories exist about the nature of consciousness. A summary of such theories will be given here. To provide some uniformity, the explanatory framework of a stack of theoretical layers is used, illustrated in the following diagram. Here, the physical biology of the brain is viewed as a substrate that might potentially be replaced or emulated via some other substrate (eg: silicon neurons). Built upon that substrate are many non-conscious processes. Either directly from the underlying substrate, or as a result of the non-conscious processes, some mental states are associated with subjective experience. The blue boxes represent potential layers involved in that subjective experience. For example, some non-conscious processes may produce representations that are associated with subjective experience. Such representations may or may not require specific functional structures in order to lead to subjective experience. Additionally, even with appropriate functional structures, something extra special may or may not be needed in order for those representations to be associated with subjective experience.

![theory layers](files/A-coherent-theory-v1-theory-layers.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Layers common to many theories of consciousness.</b> Theories of consciousness can be discussed in terms of which of these stacked layers that they focus on. Many theories focus on just one of these layers</i></li>
</ul>

**Substrate:** Everything is constructed within a physical substrate such as biological neurons. Identity Theory posits that there may be something special about biological neurons that give rise to subjective experience; or in other words, that functional isomorphisms of biological neurons such as silicon neurons or computer simulated neurons will never produce subjective experience. Inspired by the peculiar behaviour of quantum mechanics, some have suggested that the physical substrate produces quantum entanglement and that this quantum entanglement may be the underlying mechanism behind subjective experience, such as  ...{insert name of Microtubules theory}.... Such theories are quiet on which of the other layers are required. Those theories bare some similarity to that of Pan-Psychism, the theory that consciousness is fundamental in the same way that physical forces and energies are fundamental. Under Pan-psychism, some non-conscious processes lead directly to subjective experience with pan-psychist properties providing the "something special" without necessarily requiring specific representations or functional structures. However, pan-psychism is unable to explain why only certain processes are associated with subjective experience while others are not. An obvious explanation would be to require that only certain representations and associated functional structures are sufficient to "combine" the conscious properties of independent neurons / molecules / atoms, but this still leads many open questions about what those representations and/or functional structures might be that have such a power over this underlying fundamental consciousness.

**Non-conscious processes:** Many processes operate that neither directly nor indirectly lead to any kind of conscious subjective experience. While earlier work assumed that most brain processes are conscious, there is increasingly strong evidence that almost all mental processes are non-conscious, even for mental processes that are associated with attentive subjective experience. The furthest logical extension is that of epi-phenomenalism which posits that no functional mental processes are associated with subjective experience. Rather, subjective experience is posited as being some sort of non-causal summary of internal and external events. A significant problem with epi-phenomenalism is why subjective experience, and indeed consciousness at all, would have evolved if it can provide no functional / causal benefit to the individual. In that sense, epi-phenomenalism only makes any sense if viewed in conjunction with pas-psychism or with metaphysical dualistic theories where some "spirit" that exists on another plane of reality are perceiving the experiences for some grander purpose outside of the context of the corporeal reality.
- References: https://www.britannica.com/topic/philosophy-of-mind/Qualitative-states

**Representation:** Under Computational Representation Theories of Consciousness, some of those non-conscious processes produce representations that are associated with subjective experience. Debates within this area are about the kinds of these representations. For example, the intransitive vs transitive debate questions whether it is sufficient that a representation is simply held in attention or whether the representation must be in relation to the individual.......{needs work and better tie-back to actual theories}....
- References: https://www.britannica.com/topic/philosophy-of-mind/The-computational-representational-theory-of-thought-CRTT
- Reference: https://www.britannica.com/topic/philosophy-of-mind/Qualitative-states#ref283994

**Functional Structures:** The next question arises about why certain representations would have subjective experience and others would not. Is it sufficient that a particular kind of representation exists for it to be associated with subjective experience, or does that representation need to occur in conjunction with particular functional structures? For example, imagine that all aspects of the brain were understood to the point that we could identify exactly which representations are associated with subjective experience. Now imagine that an exact replica of a particular representation was encoded within the gates of a silicon memory chip within a computer. Most would argue that the silicon memory chip does not subsequently have subjective experience of that representation, because a representational state alone is insufficient for subjective experience. Some kind of functional structure presumably must be required to observe that representation. But if functional structures are indeed required, what are those functional structures? And what distinguishes those functional structures that are associated with subjective experience and those that are not? Once again here it can be hard to avoid the pitfall of infinite regress: if a representation must be interpreted in order to be perceived, and a functional structure is required in order to interpret that representation, what is the output of that interpretation if it is not just another representation needing to be further interpreted?

**Something Special:** A deeper philosophical debate rages about whether representation and functional structures alone are sufficient to produce subjective experience, or whether something else is required. In the example below, if all the representational and functional structures are in place for an individual to have both the external behaviours and internal mental behaviours of an individual in intense pain, can we even conceive it to be possible that they would not have the typical associated subjective experience of the pain?

> Suppose that, in order to avoid the risks to his patient of anaesthesia, a resourceful surgeon finds a way of temporarily depriving the patient of whatever nonfunctional condition the critic of functionalism insists on, while keeping the functional organization of the patient’s brain intact. As the surgeon proceeds with, say, a massive abdominal operation, the patient’s functional organization might lead him to think that he is in acute pain and to very much prefer that he not be, even though the surgeon assures him that he could not be in pain because he has been deprived of precisely “what it takes.” It is hard to believe that even the most ardent qualiaphile would be satisfied by such assurances.
>
> Reference: https://www.britannica.com/topic/philosophy-of-mind/Qualitative-states#ref283995

One particular area of debate falls variously under the terms "Cartesian theatre" or "homunculus problem". The Cartesian Theatre ..introduced by ....describes the interactions between supposedly two parts of the mind: the part that has subjective experience (the audience in the theatre), and the part that produces the contents of subjective experience (the actors in the theatre). This has also been described as a "homunculus" or "little man" inside the brain that has the experiences. Both descriptions pose a problem: that the audience/homonculus requires a significant level of intelligence in order to interpret and understand the show being put on; but that same intelligence is required to produce the show in the first place. In the case of homunculus, we can provide two descriptions w.r.t. to the layers described above. In the first description, the non-conscious processes produce a representation that is perceived by the homunculus. Here the problem becomes what mechanisms the homunculus uses to do that perceiving? In the second description, the representation is sufficiently processed by functional structures so that no further processing is required, and that final result is simply perceived-without-processing. In practice this raises more questions than it seeks to address. What kind of thing is it that can perceive-without-processing? In response, the theory of semiotics (discussed in a section below) claims that it is impossible for such a thing to exist, because there is no perception without interpretation, and interpretation necessarily involves processing.

_todo: references and more details from:_
- https://www.lesswrong.com/posts/GBXKZujXSZe84aAL9/the-homunculus-problem
- https://en.wikipedia.org/wiki/Homunculus
- http://pespmc1.vub.ac.be/HOMUNCUL.html
- https://www.consciousentities.com/deadends.htm
- For later: semiotics explains this as being one and the same system.

Ned Block and various other's variations propose that consciousness is divided into access consciousness and phenomenal consciousness, and that access-consciousness variously stops somewhere below the "something special" layer, with various arguments about where it stops. Such arguments down to differences in opinion about the relation between access and phenomenal consciousness: i) are they one-to-one: all access conscious events have phenomenal consciousness, ii) can some access conscious events lead to external behaviour without the individual having subjective experience?, or iii) can some phenomenal conscious events occur that have no access consciousness: for example that the individual experiences in the moment but cannot have any memory of it and cannot report on it.

# II.2 Biology and Neuroscience 

Todo - brain parts etc


---


# Part III - Problems in simple synthetic control processes

# III.1 Complexity and the need for Processing Loops

In most artificial neural network (ANN) based reinforcement learning (RL) agents today, each input is associated with an immediately produced output. This means that in an embodied agent the choice of the next physical action is made by a single pass through its ANN(s): input nodes are populated with current sensory signals, matrix operations are carried out that permute and transform those input node values through the multi-layer network of weights, and the values produced by the output nodes are immediately taken as the chosen next action. This is true even for Recurrent Neural Networks (RNN). RNNs are _recurrent_ in the sense that state from a previous pass is made available to influence the output on the next pass with the next input value. In this way, when a time-bound signal stream is fed into the RNN, it produces an output stream where each value in the output stream is influenced not just by the current input but by all inputs received up until that point. However, the RNN still produces exactly one output for every input, and each output is produced by a single pass through its network.

![single iteration anns](files/A-coherent-theory-v1-single-iteration-anns.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Single-iteration Artificial Neural Networks (ANN).</b> Each of these networks produce one output for each input, via a single pass pass through the network. In the context of an embodied agent, this means that the agent has no option for further deliberation of the same input.</i></li>
</ul>

Another form of recurrency is to execute multiple passes through the same network before producing an output. This form is common in hand-rolled algorithms, where it is usually referred to as processing _loops_. When an algorithm employs a processing loop, a single output may be produced for each input, but only after a variable length delay. Some inputs may lead to updates of internal algorithm state only, without producing an output. Or a single input may produce multiple outputs. Examples abound, but one familiar to those in the AI research community is the Expectation Maximisation algorithm. It takes as input a set of data points, produces as output a set of parameters that describes the input data set, and employs multiple iterations of alternating calculation of log-likelihood expectations and parameter optimisation. The alternating expectation and parameter optimisation loop is stopped according to a _halting rule_ that is either based on detecting diminishing returns in the improvement of log likelihoods or on completing a fixed number of iterations.

Some have begun to experiment with loops in ANNs. Complex results can be achieved with shallower networks when using a loop-style of recurrency [Kubilius et al (2019); Wen et al (2018)]. Loop architectures have been used to adaptively vary the amount of computation time allocated to problems, as Adaptive Computation Time [Graves, 2016], which has been suggested as an important component of next generation language decoder-encoders known as Universal Translators [Dehghani et al, 2018].

There is a practical limit to the complexity that a single-iteration processing architecture can achieve. The network can be made broader (more nodes in each layer) and deeper (more layers), but that increases the number of parameters that need to be optimised during learning. In earlier versions of ANNs, where smooth non-linearity functions such as sigmoid were used within hidden layers, the vanishing gradient problem [citation] meant that practical networks could not be more than a few layers in depth. Current state of the art ANNs obtain non-linearity through piecewise linear functions and enable many more layers before the vanishing gradient problem becomes an issue. However an upper bound on the number of layers is still recommended [details, and citation]. Even GPT 3 only uses 96 layers [citation].

Another problem with a single-iteration processing architecture is that its fixed depth implies a tradeoff between the maximum complexity that the architecture can handle and the cost of training in order to cater for the average complexity of situations that the agent must cope with. Additionally, if we consider that such processing may entail multiple stages of processing, the order in which those stages is executed is fixed.

An architecture that employs multiple passes through its network can be conceptualised by unrolling its iterations into a much deeper single-iteration network. But it has a number of advantages. Its depth varies dynamically as needed, for example that it is deeper for more complex problems. If processing is made up of multiple separable stages, the order in which those stages are executed can now be dynamically varied. It is additionally quite natural to imagine that for certain problems, some stages will be simply omitted entirely.

![multi-iteration ann](files/A-coherent-theory-v1-multi-iteration-ann.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Multi-iteration network.</b> Panel A: a multi-iteration network with the result from its output layer fed back as input. Panel B: an equivalent single pass network by unrolling the iterations into a deeper network assuming 3 iterations. Notice that in the depth-unrolled network, weights are shared between sections.</i></li>
</ul>

So, it can be said that there is a limit on the complexity that can be handled by a single pass through any computational process. While that computational process can be extended with more parameters, there are practical limitations to how much it can be extended. For embodied agents, this appears as a limit on the complexity of the environmental and of their own body that they can sufficiently model and respond to within a single processing iteration. In biological terms, this practical limit is manifested in terms of both the energy costs of larger brains and in terms of the time required to reach maturity of brain function.

To adapt to more complex environments, an embodied agent must employ multiple iterations of processing. This enables, for example, further analysis of the environment in order to better model its state; or further deliberation alternative action plans before proceeding. In biology, this provides scope for evolutionary pressures to trade off between a more energy hungry complex brain and a simpler less energy intensive one that might take longer to reach a decision for more complex problems. Van Bergen & Kriegeskorte (2020) make the case that recurrency is indeed employed in biology for that very reason.

_todo: Estimate of layer depth in brain?_

_todo: I use the term multi-step processing to clarify that I am referring to a particular form of recurrency where, for some materially significant portion of the process, the majority of its outcome is fed back as input.... yeesh, this is hard to quantify. _

_Todo: To avoid confusion with micro-level recurrency, this article uses the term "multi-step processing". 
or should I use multi-iteration processing everywhere instead?_

# III.2 State in a Multi-step Processor

The course taken by an agent to get from a past state to its current state is its _state trajectory_. Analogous to the path taken by an agent while walking through a maze, the state trajectory describes the path of the agent through state space. Here the state space can refer to its possible locations in physical space, such as in the maze example, or to more abstract possible states, such as an encapsulation of all measurable aspects of the agent's body parts.

Not all state trajectories are good ones. The figure below illustrates a number of possible state trajectories from start state A to goal state G, while avoiding obstacle X. Each trajectory successfully reaches the goal, but they vary in other ways that may have significant impact to the agent. They length of the trajectory may indicate energy efficiency, which is important for an agent with limited energy reserves. The length may also indicate the time taken, which impacts whether or not the goal is reached "in time". The smoothness of the trajectory can be important. A jagged trajectory might indicate that the agent's physical body is moved in a chaotic way with abrupt stops and starts, causing damage to delicate moving parts from the stresses of that chaotic movement. A smoother trajectory may be easier for the agent to subsequently learn from and reason about in order to improve its later attempts; whereas a more chaotic path may add so much noise to the observations of the trajectory that the agent is unable to detect the most important patterns for such learning.


![good and bad state trajectories](files/A-coherent-theory-v1-good-and-bad-trajectories.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Good and bad state trajectories.</b> Examples of some possible state trajectories from start state A, to goal state G, while avoiding obstacle X. The shortest and smoothest trajectory is assumed to be the best: the most energy-efficient, the quickest, the least stresses applied to the mechanics of the agent.</i></li>
</ul>

In a multi-step control process, the controller traverses a state space independent of the state of the body that it controls, as illustrated below. It needs to incorporate mechanisms to control its own state. Those mechanisms are referred to as _meta-management_, because they relate to management of the controller's own processes, rather than to management of the primary thing that the controller acts against (the agent's body in this case).

![control process trajectories](files/A-coherent-theory-v1-cp-trajectory.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Control process trajectories.</b> With multi-step processing, the control process (`CP`) has its own state trajectory (`S_cp`), influenced by its actions (`A_cp`). Control process actions only occasionally produce changes to body state (`S_bdy`).</i></li>
</ul>

Within a learning setting, the control processes must learn to manage the state of the agent's body. Typically this is influenced by feedback received in association with the outcome of some sequence of actions. That feedback must be interpreted and used to infer the best way to optimise the parameters of the control process. In a synthetic RL setting, that feedback and parameter optimisation is performed via hand-coded learning algorithms, often incorporating back-propagation and gradient descent. In a biological organism, the corresponding learning processes may be somewhat more complex and are certainly much less understood, but their effect is the same: that parameters of the control process are optimised such that future attempts would be more successful or efficient. This is a first concrete example of meta-management.

The learning processes involved with a multi-step processor may be very similar to those involved with multi-step bodily actions. Each body action plays out over time, with complex dynamics affecting the speed and trajectory taken. The body actions required to reach a particular target body state may involve the sequencing and coordination of multiple actuators or muscles. Feedback about the relative success or failure may be sparse - only received as certain points in time, with no specific details about the relative effectiveness of steps in between, and even then the meaning of the information and how it relates to the state trajectory may be ambiguous. Any learning algorithm resolves that by assuming some distribution of the affects of the feedback over the length of the state trajectory and by averaging over multiple attempts. Some of the feedback received by an agent can be more frequent and detailed, such as those produced by evolved low-level processes that encourage smooth and efficient movements.

![cp state with meta-management](files/A-coherent-theory-v1-cp-state-with-mm.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Control Process with state.</b> A control process (CP) that has state needs to act to manage its own state as well as the actions and state of the body that it controls. In some cases, this may require an additional _meta-management_ process. Some interactions omitted from the diagram for simplicity.</i></li>
</ul>

For simple control processes, those same mechanisms can be applied to parameter optimisation affecting the control process state trajectories. For example, the same low-level evolved processes can encourage efficient CP state trajectories by attempting to minimise the number of CP actions that don't produce body actions, without degrading the quality of the body state trajectories. Likewise, they may encourage "smooth" CP state trajectories in order to avoid disorganised chaotic processing.

But the simplicity of these suggested low-level CP state controls limits the capacity of the control process. Some computational problem spaces will require much more extended computational time, with much more divergent state space trajectories, than could be accompanied by these simplistic parameter optimisation rules. If a real physical world includes not just straight lines, but obstacles, walls, mazes, and other complex environmental constructions involving complex sequences of actions, then so too might a "computational world" that an advanced control process might have some need to operate within.

A good example is that of a path planner.........todo.....describe one particular planning algorithm in simple terms... Also mention that there are multiple approaches. 

![complex state trajectories](files/A-coherent-theory-v1-complex-state-trajectories.drawio.png)

* _**Complex state trajectories**. A) An example of agent position state trajectory as the agent navigates a maze environment. B) A hypothetical 2D representation of the trajectory of the internal state of a planner as it considers different possible paths for navigation within the maze environment. Here, pairs of upward and downward sub-trajectories represent the forward and backward pass of each considered path, and the general motion from lower-left to upper-right signifies the planner progressing as it finds increasingly better solutions._

The sequence of CP actions required to execute a planner are complex and, although an overall strategy is known, the exact sequence is unknown a priori. Additionally, there are multiple strategies for solving the planning problem and different domains may be better aligned to different strategies. 

So meta-management may be more than just parameter optimisation at learning time. It may involve processes for monitoring, modelling, and actively affecting the state of the control process while the control process performs its control of the body.

A detailed discussion for all the possible reasons for meta management will be carried out in a later chapter after introducing more elaborate forms of deliberative control process. For now, in the context of the control processes discussed so far, some example forms of meta management include:
- parameter optimisation at time of learning based on past action sequences and feedback 
- generation of low-level feedback signals such as to indicate efficiency and "smoothness" of state trajectories
- generation of feedback signals based on higher order understanding of the problem domain (eg: that the path planner considered paths in the wrong order)
- monitoring of control process
- active tuning of control process during execution
- strategy selection

In conclusion, a multi-step processor requires meta-management. For the most simple multi-step processors, meta-management can be in the form of simple parameter optimisation algorithms applied during a learning phase post execution. For more advanced multi-step processors, a much more advanced and active form of meta-management is required, one which might even have comparable complexity to the primary control process itself.

# III.3 Interlude: Control Options
What form might meta-management take. In order to answer that question I shall first present a review of different architectures for standard control (ie: non meta-management). 

_..todo...maybe now diagram all the component parts of an AI RL learning algorithm, so we can see what meta-management is already incorporated._

In AI, a common scenario is to train a simulated robot to navigate within a virtual environment. It is common to incorporate a Neural Network (NN) as part of the control system and to use Reinforcement Learning (RL) to train that neural network. Several broad options exist for the architecture of the control algorithm. These can be framed as a progression of improvements, that lead to increasingly better adaptability. The progress is illustrated in the diagram below, and outlined as follows. 

_...todo...update wording below to use "policy" instead of "function"._

![standard control algorithms](files/A-coherent-theory-v1-std-control-algs.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Control algorithms.</b> A graduation of increasingly adaptive control algorithms in an embodied controller. Training algorithm connectivities simplified or it is entirely omitted for the sake of simplicity. A) A so called "model-free" parameterised function that produces action without awareness of its own state. B) Model-free parameterised function that is aware of its own state. C) A so called "model-based" planner that predicts entire trajectories in order to choose the best action, with or without awareness of its own state. D) a hybrid that employs model-free control with a planner to observe and protect against major errors.</i></li>
</ul>


_..todo.. for all of the sections that follow:_
- _need to describe the algorithms better, and more formally._
- _look at industry explanations_

## Stateless reactive control process
In the most simple case, a _policy_ NN simply predicts the best action given a sensory input about the environment. A typical example is a robot car with very simple choices of actions: stop/go, left/right/forwards. The robot observes its surroundings using, for example, vision, sonar, or laser. The dynamics of the robot itself are ignored. The robot is assumed to produce the required action immediately; for example that no time is taken while the robots changes its steering angle.

This kind of agent is known as _model-free_, because it lacks an explicit model of the state-space in which it operates. For example, it has no possibility to predict expected outcomes of actions and to detect when actual outcomes diverge from expectation.

The RL algorithm is really the thing with the smarts here. It has access to much information that the agent does not. For example, it knows what the training goal is (eg: to navigate a race course), it knows the ground truth position and orientation of the robot at all times, and it knows how "costly" each robot action was relative to the goal. From that information it computes a _loss function_ as the time-devalued sum of those action costs [citation, and details]. That loss function is then used via back propagation to update the weights within the NN.

In the earlier days of neural network research, including deep learning, many of the problems addressed were of this very simple form.

## Stateful reactive control process
A small improvement enables the agent to cope with its own dynamics.

Here the agent has information about its own state. For example, information about the current steering angle and speed of the robot car's wheels. ....[citation needed].... how does that actually improve things?

A particular scalability problem afflicts reactive control processes for more realistic real-world scenarios. In the real world, the best trajectory is a function not just of the initial state, but also of the goal at the time. Here the policy NN learns a probability distribution `P(a|s,g)`. The potential range of goals could be large, and thus the dimensionality of the distribution is exponentially larger than `P(a|s)` alone. And this solution only generalises to new goals that are similar to ones seen at training time.

## Planning control process
A significant improvement to adaptability and reduction in training time is seen in AI research by incorporating a planner into the control process. Here, a model is incorporated that predicts the effect of an action on the state of the environment and on the agent's own state. Rather than predicting a single best action, the agent simulates a trajectory from its current state via a sequence of actions to see where it may end up. It does that multiple times with multiple trajectories. It completes each step by picking the best trajectory tried, and the first action from that trajectory. Then it repeats the whole process again for the next timestep.

Compared to reactive control processes. Such a solution has a significant advantage in the real world where the most appropriate action depends on the goal at the time. The planner learns a model P(s'|s,a). The model is not parameterised by goal, as the goal needs only be considered at the time of planning. Additionally, the same model parameters can be updated from experience regardless of the goal that was being followed at the time. So the model generalises well to totally unseen goals.

A key feature of this control algorithm is the use of _simulation_ when considering different possible trajectories. A simulated sequence of actions can often be run orders of magnitude faster than actually carrying out the same sequence of actions. Negative outcomes in a simulation have no impact on the agent except for the time spent running the simulation. And simulations can even be used to train other systems.......todo: [citation] example of EM where problem space is split into multiple models.

Unfortunately, this planning approach can also be computationally inefficient. In a naive implementation an extensive amount of computations are performed that are completely discarded and repeated again. It also doesn't scale well into long trajectories of high-resolution. Various optimisations exist. One particularly relevant optimisation is to use a planner to produce a course-grained high-level trajectory, and to use a reactive control process for the fine-grained motion control. In such a setup, the next point in the high-level trajectory sets a dynamic goal that is fed into the reactive control process as an additional input. [citations]

Note that the structure and algorithm of the planner is far from given. It too has many parameters. In AI research the planning algorithm is typically chosen beforehand. In a biological setting, the planning algorithm itself may be learned from experimentation and instruction.

_...todo....this needs better phrasing. I think I should accept that the planner is multi-step, and that meta-management is incorporated into the hard-coded planner implementation, but that it's all totally different for a biological setting:_
_- In most AI research today, a planning control process is still a single-step control process: each env + body state inference is associated with a single pass through its control process and an immediate choice of action. Any apparent multi-step processing, such as within the planner, are typically hard-coded by AI engineers. The planning process is not controlled by a learned NN. In biology, such a hard-coded static planner does not exist. The biological organism needs to learn how to do and control such a process itself._

_...todo...use this to introduce the idea of a biologically plausible NN-based planner._

## Observing control process
_[citations, need to find out if anyone has actually done this]_

_..todo...do I still include this?_

_...todo...make the description here more generic to refer to all versions of hybridisation:_
- just observing
- selection between full reactive and full planning
- options for the parts to learn off each other
- why this may/may not be biologically plausible for simple standard control of env+body

A different optimisation is possible that combines the best of reactive and planning control processes. I call this an _observing control process_. Here, a reactive control process is used for most actions most of the time. In parallel with that control process, a planning-based process predicts the most likely outcome given the existing trajectory being carried out by the reactive process. Those predictions are compared against the actual trajectory as it pans out. Predicted negative outcomes and major errors between prediction and observation are used to trigger a more extensive and detailed planning process to take over. Both reactive and planning processes learn their underlying models, and can help to train each other. Thus their respective behaviours converge towards each other over time.

The planning process here operates at a high-level. It can also adaptively increase or decrease its frequency of execution depending on how well the reactive process is performing. That way it can balance the needs for adaptability against the computational cost of planning.

This control process is not discussed much in the literature, but I believe it is an important control process in humans. It is also an important control process to discuss because it begins to incorporate meta-management. Here we have dynamic changes to the control process, based on observation of how the control process is performing.

## Expectation-Maximisation / Iterative Convergent Control Processes
...todo..

# III.4 Control options in Meta-management

What options are available for a control process to be meta-managed? We have already mentioned parameter optimisation. Here we shall look at some other options. The goal is not to provide an exhaustive list, but to build up a case for the need to observe the control process and to draw out what kinds of observation might be needed.

But first, to help motivate the discussion, let's first specify a useful example to have in mind....

## A Biologically Plausible Planning Control Process

The control algorithms considered were taken directly from AI research, but not all of the implementations are biologically plausible in the same form as used within AI research. It is reasonable to consider that an ability to do planning is very important for biological agents just as much as it is for artificial agents. But biological agents are unlikely to be imbued with a fully-formed pre-built planning engine. And even if such a thing was partially or fully formed, it is more likely to built using the same kind of neural network structures found throughout the rest of the brain.

![biologically plausible planner](files/A-coherent-theory-v1-bio-planner.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>A biologically plausible planner.</b> A NN-based policy executes as a multi-step control process (`CP`). CP state (`s_cp`) represents everything that a planner may need to hold onto, including the partially complete trajectory being considered at the time and information about other trajectories already attempted. The policy predicts control process actions (`a_cp`) that change the control process state (`s_cp`). CP actions sometimes also cause body actions (`a_bdy`) that lead to new body state (`s'_bdy`) and environment state (`s'_env`). One or more separately trained models could feed into the policy, or the policy itself could effectively represent those models. The training algorithm optimises policy parameters in order to achieve the right body state trajectories while meeting CP state constraints (`c_cp`) and body state constraints (`c_bdy`). Parameter optimisation of models not shown.</i></li>
</ul>

The above diagram presents a rough structure of a biologically plausible planner. The pre-built planner is replaced by a policy network that controls the behaviour of the planner in exactly the same way that a policy network could control the outwardly visible behaviour of an embodied agent. Actions produced by the planner (`a_cp`) are for the most part hidden - they modify the state of the control process (`s_cp`) without producing any outwardly visible behaviour. Depending on certain properties, some CP actions additionally produce body actions (`a_bdy`), this being the key goal of the planner.

The policy network is trained through reinforcement learning to produce suitable CP actions and body actions. On average the right body actions should be generated that produce suitable body state trajectories from current state to goal. And the number of CP actions that don't produce body actions should be minimised in such a way that the appropriateness of body actions are not significantly reduced. In a biological setting, suitable low-level constraints would be optimised through evolution in order to achieve those outcomes.

A traditional AI planner incorporates one or more models in order to predict the effect of actions. The policy network described here incorporates an inverted version of that model, and so it does not require a separate model as input. However, as mentioned in the planning control algorithm introduction above, having a separate model of state space and the effects of actions against that state space gains some efficiencies and improved generalisations. Thus a biologically plausible NN-based planner likely incorporates models too. The specifics of how that might be achieved are outside of the scope of this article.

The structure described here can produce the behaviour of a planner in the traditional AI sense. But it is more generic than that as its behaviour is driven by whatever reinforcement learning is applied against it. It could also produce other kinds of behaviour, including iteratively refined interpretation of observed state, other AI algorithms such as Expectation Maximisation (EM), or more human-like behaviours like problem solving. The same policy network could even exhibit multiple behaviours, depending on the need at the time.

But how stable would it be? Would it need a meta-management process to help it converge more rapidly?

_...todo...Also present argument that meta-management only makes sense in the context of a planner such as this, because it has multiple steps where it doesn't produce actions, whereas all the variations of reactive function always produce body actions.  That motivates why the next section considers meta-management in the context of a planner._

## Control Options

![meta-management control options](files/A-coherent-theory-v1-mm-cntrl-options.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>How meta-management can influence the control process.</b> Identification of some ways in which a control process may be meta-managed. </i></li>
</ul>


- **Parameter optimisation.** As a quick reminder, our first example of meta-management was "after the fact" parameter optimisation. This occurs as a training process guided by feedback following execution of the control process against some problem. Through processes such as gradient descent and back-propagation the parameters of the control process can be optimised so that future attempts are improved.
- **Strategy selection.** The control process may develop multiple strategies for solving different kinds of problems. For example there are multiple ways to do path planning. Selection of the most appropriate strategy for a given problem at hand is an example of meta-management. 
- **Goal selection.** Real world agents don't have hard-coded goals. They change goals according to situation.
- **Bias control.** The biological brain is believed to incorporate a predictive coding architecture....[citations, and elaboration]. Inferences incorporate biases. Those biases can be manipulated by some explicit meta-management process. An example can be seen in mammals with interactions between the sympathetic and parasympathetic nervous systems influencing thought processes.
- **Direct state control.** Perhaps it is possible to directly influence the state of the control process.
- **Input manipulation.** It is possible to change the input in order to change the behaviour of the control process. This could be, for example, through attention. A more elaborate example is to infer what input manipulations are necessary in order to produce a desired CP behaviour, which would require more advanced modelling of CP behaviours and how various inputs affect those behaviours.
- **Output manipulation.** Another possibility is to directly manipulate the output from the control process before it takes effect on other systems. One example is to use this to entirely replace the control process in some situations. Another, perhaps more realistic example, is to attenuate the strength of signals from the control process while the control process is in its earliest stages of training. When the control process is untrained, it is likely to produce chaotic behaviours that might be detrimental to the survival of the agent. Some measurement of its level of stability could be used to gradually increase the strength of its output signals over time.
- **Feedback manipulation.** The outcome of the control process causes feedback. Meta-management can be involved in the interpretation and even manipulation of that feedback. A simple example is to infer what parameter optimisations are required based on the feedback. This may include mechanisms for handling sparse feedback by somehow averaging and distributing the feedback over the sequence of actions that were carried out. Another example is to produce the feedback itself. In AI this is known as learning the reward model...[citation, and elaboration]. A further extension is possible if the meta-management processes are actually more advanced than the reactive control process, and that the meta-management processes can independently devise measures of success.
- **Controller selection.** Similar to the case of strategy selection. If multiple different control processes are available, a meta-management task is to choose which control process takes effect, or perhaps to choose a relative weighting of effect.

A few general notes can be said about the above. Firstly, parameter optimisation is the only example of an "after the fact" meta-management process. The rest all take effect during online execution of the control process against a current problem. This is significant because it suggests that a) meta-management processes need to be actively involved during execution of control processes, and b) meta-management processes need immediate live observation of the behaviour of the control process as it executes.

Many meta-management processes can be implicit or explicit. Implicit meta-management occurs as a side-effect of the reactive mechanisms of the control process. Explicit meta-management is driven by a separate process that somehow influences the control process. For example, in AI, parameter optimisation is typically carried out as an "offline" process by a learning algorithm that is entirely separate from the processes used when executing the control process. In contrast, within biological brains, it is believed that _hebbian learning_ occurs as the primary learning mechanism and that it primarily occurs as "online" learning ....[citation, and further elaboration].

---


# Part IV - Problems in complex synthetic control processes
# IV.1 Interlude: Meta-management in Deliberative Systems

_todo_

# IV.3 Meta mgmt and feedback loop options


---


# Part V - Problems in biological control processes

# V.1 Interlude: Environment, Body, and Cognitive Processes

Before continuing, it'll be useful to do a deep dive into the interactions between the three key systems of: external environment, body, and the cognitive processes. This is key because it explains the importance of independent modelling of env, body and CP, and thus why "I" becomes so prominent in the computation that produces subjective experience.

With respect to an embodied individual, three interacting systems are typically assumed: external environment, body, and mind. In an attempt to be a little more precise, this article uses the term _"cognitive processes"_ (CPs) instead of _"mind"_. In any case, while these systems provide a useful basis for discussion, their exact distinction can become blurred. So some effort is first required to define them more clearly.

## Classification

In attempting to define the delineation between these three systems, it is important to take a particular perspective: that of evolution. The most simple organisms lack a nervous system but nevertheless have evolved to react sufficiently well in order for their species to continue. We can say with some certainty that they lack cognitive processes of any sort, but they do have a clear delineation between their body and the environment around them - that delineation usually being a semi-permeable membrane. Evolutionary forces have produced such an organism to act in a way that it mostly protects itself from danger long enough to produce offspring. With some organisms, such as jelly fish, a nervous system has evolved that improves the ability of that organism to meet its homeostatic needs (eg: feeding), to protect itself from danger, and to procreate. The nervous system is entirely there to meet the needs of the body as a whole - those same same needs that applied to simple organisms lacking a nervous system. In mammals that nervous system is largely centralised into the brain as a discrete organ and in some cases that brain acts to look after its own homeostatic needs independent of the direct needs of the rest of the body.

So the nervous system is just a part of the body in the same way that a limb is. There is no need to be confused about body vs mind, as in truth the mind is just a physical brain, which is a subset of the body.

It is useful however to treat the processes and states of the brain/main as separate from the rest of the body, so they we can analyse the interactions between the subsystem of the brain/mind against the other subsystems of the rest of the body. This article draws such a distinction.

Thus we have:

||Environment|Body|Cognitive Processes (CPs)|
|--|--|--|--|
|Description|All people and things external to the physical body of the individual.|The physical form of the individual in its entirety, including head and brain.|A part of the Body. Computational processes that operate within the nervous system of the body.|
|State|Environment State|Body State|Variously referred to as Cognitive State or CP State.|
|Actions||_Body Actions_ include: any externally observable actions, hormone released from anywhere except the brain. Body Actions exclude: actions performed directly by the Cognitive Processes.|_Cognitive Actions_ include: sending nerve signals to control body actions, changing cognitive state, hormone release from the brain.|

During discussions that follow it will often be necessary to place cognitive processes and their underlying neural basis in juxtaposition with the rest of the body. In general I try to use the phrase "rest of the body" to make that clear, but in some cases that becomes too cumbersome and the term _Body_ implicitly assumes the meaning "rest of the body". This is particularly the case for the term _Body Actions_, which are always assumed to exclude those actions which are better categorised as _Cognitive Actions_.

With that said, let's examine what these three systems can do:

![env, body, and CP high level](files/A-coherent-theory-v1-env-body-cp-why.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Some pathways of how the environment, body, and cognitive processes are leveraged to support the needs of the body.</b> Solid arrows indicate the most significant relationships, with dotted arrows indicating supporting processes.</i></li>
</ul>

**Environment:**
- The environment is a source of nourishment, of pleasure, and of danger. Some components of the environment (eg: family members) have a vested interest in helping the individual. While others (eg: predators and bacteria) have a vested interest in using the body of the individual in ways that would be detrimental to the survival of the individual.
- For most interactions the size and stability of the environment outweighs any ability for the individual to manipulate it. Thus the individual may manipulate the environment in a small localised way in order to meet the needs of the individual. The individually generally does not need to also consider the homeostatic needs of the environment itself. But in some situations the individual will need to consider the environment's homeostatic needs and act accordingly; for example to water the food crops before they die, or to act nicely to other individuals in order to keep them as friends.
- In the discussions that follow, we assume a simplified model whereby the individual acts in relation to the environment only in terms of their own immediate needs. But that is not intended to imply that those other important interactions do not exist.

**Body:**
- The body's most obvious actions are those that can be externally observed, such as those it performs via its limbs. The individual performs actions using these devices and in so doing changes the state of its body, eg: the left arm was hanging down but is now held upright. In some cases the action also changes the state of the environment, eg: by pulling a fruit from a tree branch.
- The body also performs more subtle actions, including using the eye and neck muscles to change where it is looking. In that sense, body actions can be performed due to some need that is more directly related to cognitive processes than to the immediate state of the rest of the body. These range from the innocuous (eg: idly looking around while day-dreaming) to the urgent (eg: looking to see whether the unexpected sound signifies danger). Thus, the body can perform actions that influence the cognitive processes. 
- Another kind of body action is the release of hormones by endocrine glands distributed around the body. These include the thyroid, parathyroid, thymus, adrenal glands, kidneys, pancreas, ovaries and uterus (females) or testes (males). As discussed earlier, for the purposes of the discussions in this article, hormones released from the brain are arbitrarily attributed to CP rather than to the body.
- Body actions are performed for a number of broad reasons:
  - to meet immediate homeostatic needs of the body excluding cognitive processes, eg:
    - the need to maintain body temperature near an ideal level
    - the need to maintain energy levels
    - the need to sleep when the body requires it
  - to meet immediate homeostatic needs of the cognitive processes (more on this later),
  - goals that the individual has decided upon, eg:
    - to seek a partner for procreation
    - to explore the far mountain
    - to build a hut

**Cognitive Processes:**
- Arguably the most important role of CP is to take all of the raw information obtained from sensory nerves distributed throughout the body, to infer the state of the body and the state of the environment, to infer the necessary body actions to take, and to coordinate the many body muscles and organs in the execution of those body actions.
- The brain also contains endocrine glands that produce hormones and which have direct impacts on both the body and the cognitive processes. These include the hypothalamus, the pituitary gland, and the pineal gland.
- But CP is now sufficiently complex that it has its own homeostatic needs. Eg: to stop unnecessary processing in order to minimise energy usage, to avoid chaotic processing for it's negative effects on learning, to stabilise attention control. In addition, the mind is complex enough that it can execute goals that don't directly involve the body - eg: solving maths problems for the sake of learning. 
- Jobs:
  - monitor and infer body state
  - coordination of body actions
  - release of brain-based hormones
  - infer and control mind actions to meet homeostatic and goal needs of body
    - monitor and infer cognitive state
    - infer and control cognitive actions to meet homeostatic and goal needs of body
    - infer and control cognitive actions to meet the homeostatic and goal needs of the mind
- So cognitive actions are performed for a number of broad reasons:
  - to meet immediate homeostatic needs of the body excluding cognitive processes
  - to meet immediate homeostatic needs of the cognitive processes
  - goals that the individual has decided upon

## System Interactions

![env, body, and CP interactions](files/A-coherent-theory-v1-env-body-cp.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Environment, body, and CP interactions.</b> todo - description.</i></li>
</ul>

[Braindump] Points on above diagram:
- Body and CPs have their own homeostatic needs and in many cases this requires CPs to coordinate the necessary actions and other processes to meet those homeostatic needs.
- Individual's goals may relate specifically to something in the body sense (eg: "I want to move into the shade to cool down"), to something specifically in the CP sense (eg: "I want to be better at problem solving"), or have elements of both (eg: "I want to go to a cafe to study").
- The state of the environment is inferred by CPs based on information obtained via the 5 senses and from past learnings. Such inference has low certainty because the 5 senses provide only minimal information about the environment, and thus there are many hidden (unobserved) variables.
- The state of the body is inferred by CPs based on information obtained via the 5 senses, via internal senses such as proprioception and hunger, and from past learnings about the body. Such inference has a moderate certainty. The individual can act with high confidence regarding the broad high-level location and orientation of its limbs, for example. But the individual has very limited information about the internal workings of the body across its many scales. For example, often only a trained physician can identify the cause when an individual experiences pain within the area of the abdomen. There is significant room for variation on the amount of information that could be obtained about body processes, but not without cost. For example, any increase in information acquisition carries costs of a) extra energy expenditure, b) extra space taken up by additional nerves, c) extra processing capacity required in order to accurately infer based on the larger informational dimensionality. Thus the amount of information known about the body is a trade-off that is balanced through evolutionary pressures.
- The state of CPs are inferred through the meta-management feedback loop and from past learnings and modelling of the individual's own CPs. Like for the body, there is a trade-off associated with variations in the amount of information acquired about cognitive state. This is especially obvious for the case of CPs gaining information about themselves. The more CPs state that is acquired, the larger the brain must be to infer based on that acquired information. The larger and more complex the brain, the greater the amount of CPs state that might need to be acquired. Thus, necessarily, the amount of information obtained by CPs about their own processes is minimal, and the associated inference of that state carries only moderate certainty at best - something akin to knowing the broad high-level location and orientation of a limb, without knowing anything of the individual muscles, nerves, ligaments involved with holding the limb in that state.
- The level of control that the individual has over the environment is minimal, and tends to carry a high energy and time expense. Additionally, the certainty that an individual has that they will be able to obtain the desired environment state is low, eg: that a branch over their head will prevent rain from falling on them.
- The individual has full control over the state of their body, disregarding low-level molecular breakdown processes and quantum uncertainty. Most of that control is carried out automatically by the body without CPs, through localised biological processes controlled by DNA. A small selection of high order body actions are controlled by CPs. The level of energy and time expenditure associated with body control varies significantly, with most localised biological processes operating very efficiently, while some large scale outwardly observable actions requiring significant energy and time expenditure (eg: running). Those automatic biological processes probably have a fairly high accuracy in their ability to effect the desired outcome. On the other hand, the ability for CPs to control body state carries some uncertainty. Every action initiated via nerve signals from the brain occurs over a period of time and is affected by many dynamic factors, including: i) whether the individual is experiencing normal, above normal, or below normal levels of energy at the time, ii) whether some unexpected resistance is affecting movement of limbs (eg: tight fitting clothes), iii) uncertainty in the inference of body state, and iv) uncertainty in inference in the environment state that might affect body action (eg: an unseen stick that trips the individual while walking).
- The individual also has full control over the state of its CPs. Similarly for the body, many of those CPs are controlled in a localised manner. For example, there is evidence that raw optical nerve signals are initially normalised via an entirely localised process .......[citation and details]............ While the brain uses about 20% of oxygen availability within the body [citation], in general we can say that CPs are relatively effortless in comparison to large-scale body actions. Thus, it is usually more energy and time efficient to imagine many possible actions and their outcomes before choosing the appropriate body action, than it is to physically act out all of those possible actions. But CP effort is not negligeable. The level of certainty associated with a CP action is harder to quantify, but is probably relatively high.
- Environment, body, and CP state affect each other, leading to additional indirect pathways for the individual to control state of those systems. Body actions against the environment change the environment state, which can indirectly change the body state; eg: building a house to keep warm. CP actions against the body change the body state, which can indirectly change CP state, eg: turning away from a disturbing sight in order to direct attention away from it.

## Markov Process Modelling

With such significant differences between the behaviours, predictability, and level of control over each environment, body, and CP, a key feature of any models regarding them will be to distinguish between those three systems. For any given sense and associated inferred hidden state, whether that state is primarily associated with the environment, the body, or with CP has a significant impact on how certain the individual can be about the accuracy of the inference, and about the likelihood that the individual will be able to change the hidden state to a more favourable one.

That information is of particular relevance to meta-management processes. In order for meta-management processes to operate efficiently, they need to operate against a higher-order approximation. Some investigation into how those processes may operate illustrates this point.

An agent observing and interacting with an environment can model that environment with a markov process such as illustrated below in panel A: a set of `n` actions are possible, and many combinations are possible. Those actions influence `j` hidden states in complex ways. The agent make makes an observation containing `q` features. Each of those components have very large cardinality. Each individual effector nerve in the individual can lead to a range of actions in continuous space, depending on the frequency and strength of its spike train. The observations are made up of thousands of individual nerve signals. And the hidden state space is potentially enormous. For training of fine-controlled muscle movement, with actions inferred from high precision visual, tactile, and audial sensory information, that scale of modelled markov process is appropriate. However, for higher-order monitoring, rationalisation, and inference of required adjustments, that level of granularity would overwhelm any process that needs to simulate outcomes and consider alternatives. More importantly, that fine-grained level of granularity is unnecessary.

Thus, it is more efficient to apply a high-order re-modelling. In a slight simplification, that can be conceived of as grouping the low-level parts of the modelled markov process as indicated in panel B. That results in a much simpler higher-order model, represented in panel C. For example, our touch sense is really millions of independent nerve endings, but within our conscious perception we think about touch as a single state space with location, texture, amplitude and valence being parameters instead of low-level individual points.

![markov modules](files/A-coherent-theory-v1-markov-modules.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Markov process modelling.</b> Actions `a`, hidden state `h`, and observations `o`. Lower-case letters refer to low-level fine-granularity, while upper-case letters refer to higher-order abstractions. `m << n`, `k << j`, and `p << q`.</i></li>
</ul>

As an aside: its worth noting that it is likely not just meta-management processes but higher-order planning as a whole that operates at this higher-order level. So while precise control of limb movement, for example, may require the more detailed modelling; overall coordination of actions across the body and CP probably operate a higher-order level.

## Identification of Source

So, meta-management operates against a higher-order representation of action, observation, and hidden state spaces. It is also needs to partition those spaces by their mutual level of interaction (aka mutual information).

Actions in the CP space have no impact on the environment. Actions in the body space influence the environment and the body, and have varying levels of influence over the CP space. Thus, when inferring the best action to take to apply a correction against a non-ideal state, the individual needs to model which modalities of actions are effective against different modalities of state.

Thus the higher-order representations have a strong segmentation between environment, body, and CP in both state and action spaces. Effectively, any sensory information that reaches the attention of higher-order executive and meta-management processes will have a sort of labelling associated with it to identify its source. Events that occur in the environment are labelled as occurring external to the body. Events that occur within the body are labelled as such. Events that occur within CPs are correspondingly labelled.

## Schema

_todo: flesh out:_
- body schema
- cognitive schema


## The Ego in "I"
_todo_

In combination with body schema and cognitive schema, labelling of events leads to a strong association between an event and the schema of its source. So, when I think a thought and subsequently think about having that thought, I know that "I" had that thought.

# V.3 Deeper Dive into Meta-management

In a section above I introduced the idea of meta-management and looked at some of the possible architectures, but I was unable to clearly articulate why one possible architecture is really any different to another. I shall now explain that, by looking more closely at the mechanisms of control and learning.

## Habitual Standard Control Management

Consider the processes required to manage body actions, in what shall here be referred to as standard control management in order to distinguish from meta-management. In the simplest form, a neural network learns a function that maps an action signal from the difference between the desired and actual states. Illustrated below, this is the basis for a habitual system - one that slowly learns to act appropriately for its own advantage over many trial-and-error executions. It lacks any understanding of the mechanisms underlying the environment in which it operates. It lacks the ability to predict accurately in an entirely novel situation. And it lacks the ability to adapt rapidly, should the environment suddenly change in a substantial way.

![Habitual standard management](files/A-coherent-theory-v1-habitual-std-mgmt.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Habitual System.</b> A neural network predicts the best action given the difference between desired state and actual state. Prediction errors spontaneously cause updates to the relative strengths of neuronal connections.</i></li>
</ul>

Such a system can be extended, for example, to contain an explicit model of the environment and to explicitly model the behaviours and limitations of its own body. With more direct modelling of those independent systems, the system has a greater ability for generalisation, and thus a greater ability to adapt to novel situations. But the system is still fundamentally a habitual one that learns through exposure to many repeated trial-and-error executions.

Learning in such a system is carried out in a relatively well understood way. Prediction errors and positive and negative rewards are fed through the system in the reverse direction (ie: from output layer towards input layer) and they adjust the relative weighting between different neuronal connections. Subsequently, outputs that were associated with more negative outcomes are less likely to occur and outputs that were associated with more positive outcomes are more likely to occur for the same input. The interpretation of feedback signals and application of those learning mechanisms can be orchestrated through innate processes that evolve over thousands and millions of years.

Learning pressure received by the individual may be in the form of pure positive and negative rewards such as satiation of hunger through the finding and consumption of food and such as the pain associated with bodily injury.

Several researchers have suggested that a learned reward system may be layered on top, based on the higher-order modelling of the environment discussed above......[citations, incl. Damasio's book on emotion]........[todo: details]....Those learned feedbacks are particularly useful because they act to shorten the feedback horizon ...[needs explanation]... - by modelling what actions will ultimately lead to future positive and negative rewards, the learned processes provide more frequent feedback than the environment alone. An example is via hormones produced by the endocrine system such as associated with anticipation of reward. This is known to affect learning...[citation]......

_todo: I think this needs to elaborate more on how this system learns and converges, and in particular how it is "inherently convergent":_
- simple innate learning pressure
- active inference as a way of balancing exploration vs exploitation
- simple modelling as a predictive process (inherently convergent)
- simple modelling as input into active inference, and thus providing higher-order learned reward system

## Rational Standard Control Management

Anecdotally, humans seem to possess an additional system that operates in an entirely different way. One which in some cases can produce suitable behaviour from only a single observation of another's behaviour. For shall refer to this as the _rational system_.

We are still a long way from understanding how this system operates, so in order to motive some of the discussion that follows I shall hazard a guess at some of its features, illustrated in the diagram below. As discussed above already, the CPs must utilise multi-step processing, and I believe that is a key part of the rational system. As part of that multi-step processing, I suspect it also learns (likely in a habitual way) a repertoire of strategies for navigating possible CP state paths given different problem domains. It clearly incorporates both short term and long term memories. It also incorporates modelling of other systems, in a way that somehow can incorporate new information from few or even a single example. I believe a key part of that few-shot modelling is based on building up randomly accessible associative relations of higher-order abstract concepts. For example that rain and wetness are associated. And for example, given that relation and a few others, that it is possible to trick another individual into thinking it is raining by spraying water from a hose over their head while they are looking away.

![Rational standard management](files/A-coherent-theory-v1-rational-std-mgmt.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Rational System.</b> Some of the features that might underly rational thought.</i></li>
</ul>

For an individual that lives in a complex environment, the state space trajectories being managed include simple spontaneous sense-action events, and long chains of observation collection, consideration, planning, and sequences of actions. The latter requires rational management in order to cope with the dynamic nature of the environment, and to be able to act on new information after receiving it only once. It also requires rational management in order to cope with complex abstractions in the environment (eg: labels, logic, etc).

_todo: elaborate more on convergence of rational systems:_
- learned learning pressure
- unlikely to have innate pressures
- learning predictive modelling (inherently convergent)
- but few-shot processes may be inherently unstable
- very complex, so large neuronal mass in brain (lots of training required)
- all closely related to why human brain takes so long to mature

## Habitual and Rational Meta-management

What kinds of processes are meta-management made from? Suppose the brain was divided discretely into a habitual region and a rational region, which regions are required for meta-management?

**Habitual processes:**
- The concept of a state-space trajectory planner was proposed earlier. This applies to both standard control (eg: of body state) and meta-management (ie: of CP state).  Largely this can be seen as repeatable strategies for solving problems in different domains, rather than the direct generation of the solutions themselves. As such, the planner is relatively stable and needs only learn slowly. Habitual processes are sufficient. This has the advantage that learned strategies are executed efficiently. Habitual processes also tend to be inherently convergent.

**Rational processes:**
- Meta-management requires domain knowledge to understand even simple things like whether the current thought process is productive. That domain knowledge is largely contained within the rational system.
- Rational processes enable more advanced modelling of one's own mental abilities, plus comparison to others. This motives the individual to improve one's own mental strategies, and thus acts as a powerful learning driver.
- The modeling capabilities of rational processes are required in order to be able to relate external environment events (eg: going to school) to how they can improve one's own mental abilities.
- The few-shot capabilities of the rational processes enable monitoring of newly attempted mental strategies within standard control processes. That is further achieved through access to the same domain knowledge and modelling as used within standard control processes.

The complexity of the rational processes suggests that a large neuronal mass must be required for it to operate. The overlap of domain knowledge required by by both standard control and meta-management processes suggests that they must share access to the same repository system. All up, it seems unlikely that meta-management could evolve in animals as an independent process. The extra neuronal mass required for meta-management to employ its own independent rational system is too energy inefficient to be evolutionarily likely. The complexity of sharing a domain knowledge repository between two independent target systems also seems unlikely. Rather, meta-management must be a fully integrated / inline process with standard control processes.

## Convergence of Meta-Management

The rational meta-management process still appears fundamentally unstable. What learning pressures make it converge towards stability and utility? At this stage I can only offer a few suggestions.

_todo: the description of "narrowing convergent boundaries" and how I've used it aren't quite aligned._

_todo: also describe habitual and rational systems as being part of an adversarial co-training system - the level to which they are co-convergent indicates how accurate or inaccurate they are. This is probably the first thing to mention. And then go into the narrowing convergent boundaries are a more specific concrete description of that._

I suspect that interactions between the habitual system, the rational system, and pressures from the external environment, create an effect of narrowing convergent boundaries. With respect to a single learning system, negative and positive feedback from its surroundings causes it to converge towards an optimum behaviour, but that convergence may be slow. If, at the same time, a second system builds up a model of the surroundings and its feedbacks, and if it applies additional feedback on the first system, then it applies a kind of an additional soft boundary against that first system. More importantly, as that second system gradually improves its model, the boundary that it applies to the first also converges. Thus, the range of behaviours of the first system that are allowed by the second system gradually narrows over time.

Panel A in the diagram below illustrates how the external environment and the rational system may act to apply narrowing convergent boundaries against the learning within the habitual system. Feedback signals received from the environment apply an after-the-fact convergent pressure, in the form of negative and positive rewards and in the form of prediction error. When the rational system is operating near its optimum it provides a before-the-fact convergent pressure, in the form of prevention (usually to avoid a catastrophic error occurring) and in the form of encouraging exploration. For example, the individual may have developed a habit of walking a particular route through the forest to the nearest watering hole, but the rational system will prevent that habituated path because it remembers that a tree had fallen and blocked the path at a location that is currently out of sight.

![Narrowing convergent boundaries](files/A-coherent-theory-v1-mm-convergence.drawio.png)

<ul>
<li style="list-style-type: none;"><i><b>Narrowing convergent boundaries.</b> ...todo...</i></li>
</ul>

When the rational system is not operating near its optimum, the environment and habitual system act to apply a narrowing convergent boundary, illustrated in Panel B. The external environment applies after-the-fact feedback in the form of positive and negative rewards that the rational system can use for future improvement through both rational adaptation and through whatever underlying learning mechanisms it operates on. As actions become more strongly habituated, it becomes harder for the rational system to counteract them, thus applying a before-the-fact prevention against the most catastrophic errors.

The modalities of environmental pressures requires some further discussion. Discussion in a section above already mentioned pure positive and negative rewards such as hunger satiation and pain. It also mentioned learned higher-order feedback mechanisms that layer on top of those innate learning mechanisms.

One further learning layer exists on top. With such advanced and flexible learning and meta-management comes a high degree of freedom. For example, many different modes of behaviour are potentially equally effective. Most negative short-term outcomes can be adapted to, so that the total outcomes is net positive. This can lead to a high degree of instability - if so many modes of behaviour are equality advantageous, there is little learning pressure to maintain a consistent mode of behaviour. In turn, that can lead to constant re-learning within the habitual system. Ultimately, the individual may converge extremely slowly towards its most optimum set of behaviours. It has been suggested...[citations, including Damasio's emotion book].... that society plays an important role here.

Effectively, society is a collective behavioural exploration and learning system that spans a longer time scale than that of an individual. There are many possible systems of societal norms, and the societies that thrive will tend to have produced systems of societal norms that benefit the society and the individuals within it and those systems will tend to remain relatively stable over time. Through constant feedback from adults and peers, a naive individual's higher order processes are trained to produce behaviours that are consistent with the norms of the society in which they live. That societal pressure thus provides a significant portion of the narrowing convergent boundary exerted by the environment on the individual's learning, particularly for its rational systems including meta-management.

# V.4 Semiotics

_todo_


---


# Part VI - Solution

_todo_


---


# Part VII - The Intuitional Gap

_todo_


---


# Part VIII - Predictions

_todo_


---


# Part IX - Next Steps

_todo_
