This is an attempt to collect together the hidden guiding principles that I've been applying in trying to formulate my theories.

### If it can operate without something, then it probably does
This is a special case of Occam's razor. On the face of it, a system that needs to model a particular behaviour should have a component that addresses that behaviour, or that behaviour may fail to emerge. But, in practice, it's often the case that a simple architecture that is generic enough can be bent in a way to produce unexpected behaviours.

This principle is based on many experiences in the scientific and even IT communities, that simpler solutions often lead to more powerful and adaptable outcomes. A possible explanation, in the context of AI, is that more complex systems can have more false starts, whereas a simpler system will need less training time.

Example:
> We appear to experience the conclusions of decisions. Does that mean that we _experience_ outputs, as well as inputs? However, it's also possible that the only _experience_ of conclusions is via those conclusions circling back around as data that is passed into working-memory and then received as inputs. Based on the principle of 'if it can operate without something, then it probably does', it's likely that we _do not_ directly experience any outputs.
